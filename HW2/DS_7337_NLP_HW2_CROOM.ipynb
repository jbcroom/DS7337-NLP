{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DS 7337 - Natural Language Processing\n",
    "\n",
    "### Author: Brandon Croom\n",
    "\n",
    "### Homework: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "*** Introductory Examples for the NLTK Book ***\nLoading text1, ..., text9 and sent1, ..., sent9\nType the name of the text or sentence to view it.\nType: 'texts()' or 'sents()' to list the materials.\ntext1: Moby Dick by Herman Melville 1851\ntext2: Sense and Sensibility by Jane Austen 1811\ntext3: The Book of Genesis\ntext4: Inaugural Address Corpus\ntext5: Chat Corpus\ntext6: Monty Python and the Holy Grail\ntext7: Wall Street Journal\ntext8: Personals Corpus\ntext9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
    }
   ],
   "source": [
    "# import nltk and other items\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.book import *\n",
    "from nltk.corpus import words\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "import numpy as np\n",
    "\n",
    "# load the same corpus used in HW1\n",
    "corpus_root = \"C://RAI//DS7337-NLP//HW1//corpus\"\n",
    "file_pattern = r\".*/*.*\\.txt\"\n",
    "ptb = PlaintextCorpusReader(corpus_root,file_pattern)\n",
    "ptb.fileids()\n",
    "\n",
    "# define a method that will get us all words in the english language\n",
    "def english_lang_words():\n",
    "    word_list = words.words()\n",
    "    return len(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.\tIn Python, create a method for scoring the vocabulary size of a text, and normalize the score from 0 to 1. It does not matter what method you use for normalization as long as you explain it in a short paragraph. (Various methods will be discussed in the live session.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.07159029467423628"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# define the vocabulary size method. Filter out everything that is not alpha numeric and make it all lower case.\n",
    "# We'll normalize using the size calculation of the vocabulary and divide by the total number of the vocabulary size. Using the standard normalization formula (x - min(x) / (max(x) - min(x))).\n",
    "# Assuming min(x) = 0 (this is the fewest number of words possible) this minimizes to x/max(x) where x=size of the text after cleansing and max(x) = the full size of the english language \n",
    "def s_vocab_size(text):\n",
    "    size = len(set(word.lower() for word in text if word.isalpha()))\n",
    "    return size/english_lang_words()\n",
    "\n",
    "s_vocab_size(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[1.         0.34073067 0.113989   0.51548495 0.24108302 0.06354701\n 0.51542313 0.         0.34178154]\n"
    }
   ],
   "source": [
    "# define the vocabulary size method. Filter out everything that is not alpha numeric and make it all lower case. This function is similar to the example provided in class. It allows us to\n",
    "# take multiple text and analyze at once. We'll normalize this result using the scikit-learn minmax_scale method which will get us between 0,1 return values\n",
    "# We'll normalize using the size calculation of the vocabulary and divide by the total number of the vocabulary size. Using the standard normalization formula (x - min(x) / (max(x) - min(x))).\n",
    "# Assuming min(x) = 0 (this is the fewest number of words possible) this minimizes to x/max(x) where x=size of the text after cleansing and max(x) = the full size of the english language \n",
    "def n_vocab_size(*arg):\n",
    "    vocab_size = np.array([])\n",
    "    \n",
    "    #### Getting the Vocab Size\n",
    "    for text in arg:\n",
    "        vocab_size = np.append(vocab_size,len(set(word.lower() for word in text if word.isalpha())))\n",
    "    \n",
    "    #### Normalizing using sklearn preprocessing \n",
    "    vocab_size_norm_sklearn = minmax_scale(vocab_size, feature_range=(0,1), axis=0)\n",
    "    \n",
    "    return(vocab_size_norm_sklearn)\n",
    "\n",
    "vocab_size = n_vocab_size(text1,text2,text3,text4,text5,text6,text7,text8,text9)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\tAfter consulting section 3.2 in chapter 1 of Bird-Klein, create a method for scoring the long-word vocabulary size of a text, and likewise normalize (and explain) the scoring as in step 1 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "8.025817788591511e-05"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# define the long word score function. We'll allow the user to input the word length and the number of times the word should occur in the text before it's counted. We'll normalize by \n",
    "# dividing the length of the long words by the total number of words in the text provided. Using the standard normalization formula (x - min(x) / (max(x) - min(x))). Assuming min(x) = 0 (this is the \n",
    "# fewest number of words possible) this minimizes to x/max(x) where x=size of the text after cleansing and max(x) = the full size of the english language \n",
    "def s_long_word_score(text, word_len=7, word_freq=7):\n",
    "    fdist = FreqDist(text)\n",
    "    size = len(sorted(word for word in set(text) if len(word) > word_len and fdist[word] > word_freq))\n",
    "    return size/english_lang_words()\n",
    "\n",
    "s_long_word_score(text5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.82779456 0.59214502 0.06193353 1.         0.01812689 0.00755287\n 0.54682779 0.         0.14199396]\n"
    }
   ],
   "source": [
    "# define the long word score function. We'll allow the user to input the word length and the number of times the word should occur in the text before it's counted. Similarly to the vocab_size\n",
    "# method we'll define this method to take multiple texts at once and leverage the min_max_scaler from SKLearn to do the scaling.\n",
    "def n_long_word_score(*arg, word_len=7, word_freq=7):\n",
    "    long_word_score = np.array([])\n",
    "    \n",
    "    #### Getting the Vocab Size\n",
    "    for text in arg:\n",
    "        fdist = FreqDist(text)\n",
    "        size = len(sorted(word for word in set(text) if len(word) > word_len and fdist[word] > word_freq))\n",
    "        long_word_score = np.append(long_word_score,size)\n",
    "    \n",
    "    #### Normalizing using sklearn preprocessing \n",
    "    long_word_score_norm_sklearn = minmax_scale(long_word_score, feature_range=(0,1), axis=0)\n",
    "    return(long_word_score_norm_sklearn)\n",
    "\n",
    "word_score = n_long_word_score(text1,text2,text3,text4,text5,text6,text7,text8,text9)\n",
    "print(word_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\tNow create a “text difficulty score” by combining the lexical diversity score from homework 1, and your normalized score of vocabulary size and long-word vocabulary size, in equal weighting. Explain what you see when this score is applied to same graded texts you used in homework 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Text Difficulty Score The Ontario Readers: Third Book:  0.04969884364250884\nText Difficulty Score The Ontario Readers: Fourth Book:  0.05454543644267229\nText Difficulty Score The Ontario Readers: The High School Book:  0.05382365014314133\n"
    }
   ],
   "source": [
    "# HW 1 lexical diversity function\n",
    "\n",
    "# define the text word count\n",
    "def text_word_count(text_data):\n",
    "    return len(text_data)\n",
    "\n",
    "# define the text vocabulary size\n",
    "def text_vocab_size(text_data):\n",
    "    return len(set(text_data))\n",
    "\n",
    "# define lexical diversity as it was defined in HW1\n",
    "def lexical_diversity(text_data):\n",
    "    word_count = text_word_count(text_data)\n",
    "    c_vocab_size = text_vocab_size(text_data)\n",
    "    diversity_score = c_vocab_size / word_count\n",
    "    return diversity_score\n",
    "\n",
    "# text difficulty score function. Create a weighted average function. Allow the user to input specific weights for\n",
    "# lexical diversity, word score, and vocabulary size. Defaults for all are 1 to keep the weights equal\n",
    "def text_diff_score(text, lex_div_weight=1,word_score_weight=1,vocab_size_weight=1):\n",
    "    lex_div = lexical_diversity(text)*lex_div_weight\n",
    "    word_score = s_long_word_score(text) * word_score_weight\n",
    "    size_vocab = s_vocab_size(text) * vocab_size_weight\n",
    "\n",
    "    return (lex_div + word_score + size_vocab)/3\n",
    "    \n",
    "# print out the new text difficulty scores\n",
    "print(\"Text Difficulty Score The Ontario Readers: Third Book: \",text_diff_score(ptb.words(ptb.fileids()[0])))\n",
    "print(\"Text Difficulty Score The Ontario Readers: Fourth Book: \",text_diff_score(ptb.words(ptb.fileids()[1])))\n",
    "print(\"Text Difficulty Score The Ontario Readers: The High School Book: \",text_diff_score(ptb.words(ptb.fileids()[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Text Difficulty Score The Ontario Readers: Third Book:  0.038337383337922006\nText Difficulty Score The Ontario Readers: Fourth Book:  0.19243566947386515\nText Difficulty Score The Ontario Readers: The High School Book:  0.7013226926771597\n"
    }
   ],
   "source": [
    "# text difficulty score function. Create a weighted average function. Allow the user to input specific weights for\n",
    "# lexical diversity, word score, and vocabulary size. Defaults for all are 1 to keep the weights equal\n",
    "def n_text_diff_score(*args, lex_div_weight=1,word_score_weight=1,vocab_size_weight=1):\n",
    "\n",
    "    lex_div_score = np.array([])\n",
    "\n",
    "    #### Getting the lexical diversity\n",
    "    for text in args:\n",
    "        lex_div_score = np.append(lex_div_score,lexical_diversity(text))\n",
    "\n",
    "    word_score = n_long_word_score(*args)\n",
    "    vocab_size = n_vocab_size(*args)\n",
    "\n",
    "    lex_div_score = lex_div_score*lex_div_weight\n",
    "    word_score = word_score* word_score_weight\n",
    "    vocab_size = vocab_size * vocab_size_weight\n",
    "\n",
    "    text_diff_score = (lex_div_score + word_score + vocab_size) / 3\n",
    "\n",
    "    return text_diff_score\n",
    "\n",
    "tdiff_score = n_text_diff_score(ptb.words(ptb.fileids()[0]),ptb.words(ptb.fileids()[1]),ptb.words(ptb.fileids()[2]))\n",
    "\n",
    "# print out the new text difficulty scores\n",
    "print(\"Text Difficulty Score The Ontario Readers: Third Book: \",tdiff_score[0])\n",
    "print(\"Text Difficulty Score The Ontario Readers: Fourth Book: \",tdiff_score[1])\n",
    "print(\"Text Difficulty Score The Ontario Readers: The High School Book: \",tdiff_score[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis Notes: \n",
    "Methods for this exercise were generated in two ways: one allowing for the analysis of a single text and one allowing for analysis of multiple texts at once. The single text methods scaled data based on words in the english language. This was done since there isn't an easy way to scale single values between 0 and 1 except by a percentage based methods (Min_Max_Scalars won't calculate). The multiple text approach allows for the use of the sci-kit learn min_max_scalars method to performthe scaling. Though the approach in these methods differ we do see similarities in the output results as noted below. \n",
    "\n",
    "Using the Single Text Methods:\n",
    "Using the text difficult score above for the single text methods, we see that this text difficulty score follows a similar pattern to just looking at lexical diversity for the texts, like in HW1. It's a bit counter intuitive. The text designed for high school has a score of 0.053, which is lower than the book for fourth grade. We would expect that the high school book would have the higher of the scores when compared to books for third and fourth graders. The text difficulty scores for the third and fourth grade books show they are close in difficulty, but do have a separation which would be indicative of the grade change.\n",
    "\n",
    "Using the Multi-Text Methods:\n",
    "The text designed for high school has a score of 0.701 which is a higher difficulty than the other two books. This is intuitive as we would expect high school books to have more difficult text when compared to third and fourth grade books. We also see a difference between the third and fourth grade books with scores of 0.038 and 0.19 respectively. Again this seems intuitive due to the expectation that as you increase in grade level the difficulty of text should increase. The over all progression of the text feels much more intuitive in this approach than with the single text approach."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-candidate"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bit36305f9ad6c54384bcdc334bae9910c8",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}