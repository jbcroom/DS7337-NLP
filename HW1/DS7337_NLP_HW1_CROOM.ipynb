{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DS 7337 - Natural Language Processing\n",
    "\n",
    "### Author: Brandon Croom\n",
    "\n",
    "### Homework: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.\tInstall Python (if you don’t have it already), and install NLTK.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version:  3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]\n",
      "NLTK Version:  3.4.5\n"
     ]
    }
   ],
   "source": [
    "#Import the required libraries\n",
    "import sys\n",
    "import nltk\n",
    "\n",
    "#print out the versions to show the installation of Python and NLTK\n",
    "print(\"Python Version: \",sys.version)\n",
    "print(\"NLTK Version: \", nltk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Follow the instructions in chapter 1 of Bird-Klein for implementing a “lexical diversity” scoring routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "#Load in the books\n",
    "from nltk.book import *\n",
    "\n",
    "#define the lexical diversity scoring routine\n",
    "def lexical_diversity(text_data):\n",
    "    word_count = text_word_count(text_data)\n",
    "    vocab_size = text_vocab_size(text_data)\n",
    "    diversity_score = vocab_size / word_count\n",
    "    return diversity_score\n",
    "\n",
    "#define the text word count\n",
    "def text_word_count(text_data):\n",
    "    return len(text_data)\n",
    "\n",
    "#define the text vocabulary size\n",
    "def text_vocab_size(text_data):\n",
    "    return len(set(text_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification Test from Chapter:\n",
      "Lexical Diversity Text3:  0.06230453042623537\n",
      "Lexical Diversity Text5:  0.13477005109975562\n",
      "\n",
      "Additional Test:\n",
      "Lexical Diversity Text4:  0.06617622515804722\n",
      "Lexical Diversity Text6:  0.1276595744680851\n"
     ]
    }
   ],
   "source": [
    "#test the lexical diversity function\n",
    "#do the initial test with the text references in the chapter to verify functionality\n",
    "print(\"Verification Test from Chapter:\")\n",
    "print(\"Lexical Diversity Text3: \",lexical_diversity(text3))\n",
    "print(\"Lexical Diversity Text5: \",lexical_diversity(text5))\n",
    "print()\n",
    "#secondary test just to verify additional texts\n",
    "print(\"Additional Test:\")\n",
    "print(\"Lexical Diversity Text4: \",lexical_diversity(text4))\n",
    "print(\"Lexical Diversity Text6: \",lexical_diversity(text6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.\tGo to http://www.gutenberg.org/wiki/Children%27s_Instructional_Books_(Bookshelf), and obtain three texts (of different grade levels) from the “Graded Readers” section. Report the lexical diversity score of each. Explain whether the result was surprising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['18561.txt', '18702.txt', '19923.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the custom reader to read the downloaded Gutenberg books. The books downloaded were:\n",
    "#The Ontario Readers: Third Book (18561.txt)\n",
    "#The Ontario Readers: Fourth Book (18702.txt)\n",
    "#The Ontario Readers: The High School Reader (19923.txt)\n",
    "from nltk.corpus import BracketParseCorpusReader\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = \"D://Users//jbcroom1//Documents//GitHub//DS7337-NLP//HW1//corpus\"\n",
    "file_pattern = r\".*/*.*\\.txt\"\n",
    "#ptb = BracketParseCorpusReader(corpus_root,file_pattern)\n",
    "ptb = PlaintextCorpusReader(corpus_root,file_pattern)\n",
    "ptb.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Diversity The Ontario Readers: Third Book:  0.11501215001376602\n",
      "Lexical Diversity The Ontario Readers: Fourth Book:  0.12121775025799794\n",
      "Lexical Diversity The Ontario Readers: The High School Book:  0.1039680780314786\n"
     ]
    }
   ],
   "source": [
    "print(\"Lexical Diversity The Ontario Readers: Third Book: \",lexical_diversity(ptb.words(ptb.fileids()[0])))\n",
    "print(\"Lexical Diversity The Ontario Readers: Fourth Book: \",lexical_diversity(ptb.words(ptb.fileids()[1])))\n",
    "print(\"Lexical Diversity The Ontario Readers: The High School Book: \",lexical_diversity(ptb.words(ptb.fileids()[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lexical diversity of these books was suprising. We see that the book designed for high school has less lexical diversity than the books designed for younger readers. It's diversity is 10.4%. The book designed for fourth year has the highest diversity at 12.12%, while the book designed for third year has a diversity of 11.5%. This goes against the expectation that as a student increases in grade level the reading diversity should increase. In this particular instance we see a fourth year is exposed to greater lexical diversity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Also compare the vocabulary size of the same three texts. Explain whether the result was surprising.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: The Ontario Readers: Third Book:  9608\n",
      "Word Count: The Ontario Readers: Third Book:  83539\n",
      "Vocabulary Size: The Ontario Readers: Fourth Book:  11746\n",
      "Word Count: The Ontario Readers: Fourth Book:  96900\n",
      "Vocabulary Size: The Ontario Readers: The High School Book:  16415\n",
      "Word Count: The Ontario Readers: The High School Book:  157885\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary Size: The Ontario Readers: Third Book: \",text_vocab_size(ptb.words(ptb.fileids()[0])))\n",
    "print(\"Word Count: The Ontario Readers: Third Book: \",text_word_count(ptb.words(ptb.fileids()[0])))\n",
    "\n",
    "print(\"Vocabulary Size: The Ontario Readers: Fourth Book: \",text_vocab_size(ptb.words(ptb.fileids()[1])))\n",
    "print(\"Word Count: The Ontario Readers: Fourth Book: \",text_word_count(ptb.words(ptb.fileids()[1])))\n",
    "\n",
    "print(\"Vocabulary Size: The Ontario Readers: The High School Book: \",text_vocab_size(ptb.words(ptb.fileids()[2])))\n",
    "print(\"Word Count: The Ontario Readers: The High School Book: \",text_word_count(ptb.words(ptb.fileids()[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Write a paragraph arguing whether vocabulary size and lexical diversity in combination could be a better measure of text difficulty (or reading level) than either measure is by itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In looking at the data provided it makes sense to combine the vocabulary size and lexical diversity as a measure of text difficulty. Using the examples above as a baseline, we've already shown that just looking at lexical diversity has interesting results. We see from those initial comparisons that the book designed for high school has a lower lexical diversity when compared to the other books. These resuls are a bit counter intuitive. However, looking at the vocabulary size we see that the high school book has both a larger word count and a larger vocabulary size when compared to the other two books. This logically maskes sense as we would expect a high school based book to have a larger range of vocabulary. Taking both the lexical diversity and the vocabulary together is a much more adequate reference of text difficulty. We can have a book that has lower lexical diversity but higher vocabulary size. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
